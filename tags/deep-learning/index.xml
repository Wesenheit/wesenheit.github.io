<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on Wesenheit</title>
    <link>https://wesenheit.github.io/tags/deep-learning/</link>
    <description>Recent content in Deep Learning on Wesenheit</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Mar 2023 15:00:00 +0100</lastBuildDate>
    <atom:link href="https://wesenheit.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GMVAE clustering applied to RNA sequencing</title>
      <link>https://wesenheit.github.io/posts/2023-03-19-gmvae-clustering-applied-to-rna-sequencing/</link>
      <pubDate>Sun, 19 Mar 2023 15:00:00 +0100</pubDate>
      <guid>https://wesenheit.github.io/posts/2023-03-19-gmvae-clustering-applied-to-rna-sequencing/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;One of many classical tasks of machine learning is clustering, based on the data one would like to distinguish some clusters.&#xA;There are many classical approaches to clustering, the most notable ones include: K-means, GMM (Gaussian Mixture Model) trained with the EM algorithm, and DBSCAN.&#xA;While each of those algorithms is widely used in research and industry,&#xA;all of them try to cluster the data using its original representation and mostly fail in the case of hidden similarity between observations.\&#xA;In this post, I present a GMM+VAE deep learning architecture, which will be used to cluster the data based on a learned embedding of the data.&#xA;The clustering model is based on &lt;a href=&#34;https://arxiv.org/abs/1611.02648&#34;&gt;this paper&lt;/a&gt;, while application to RNA sequencing data is based on the work that I performed during my studies at the Warsaw University.&#xA;The original project with a solution can be found in a related &lt;a href=&#34;https://github.com/Wesenheit/SAD2-22W/tree/main/Project1&#34;&gt;GitHub repository&lt;/a&gt;.&#xA;The dataset used in the study was taken from the &lt;a href=&#34;https://openproblems.bio/competitions/neurips_2021/&#34;&gt;NeuroIPS 2021&lt;/a&gt; competition. In this blog, we will tackle the joint&#xA;embedding part of the competition. We will try to embed biological information in an unsupervised manner and at the same time reduce the impact of batch effect on&#xA;the model performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LSTM vs Tolkien</title>
      <link>https://wesenheit.github.io/posts/2023-02-02-lstm-tolkien/</link>
      <pubDate>Thu, 02 Feb 2023 23:20:00 +0100</pubDate>
      <guid>https://wesenheit.github.io/posts/2023-02-02-lstm-tolkien/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;p&gt;This notebook is greatly inspired by &lt;a href=&#34;https://gbnegrini.com/post/tolkien-character-or-prescription-drug-neural-networks/&#34;&gt;this notebook&lt;/a&gt; by G. Negrini. Please check out his website, as he make really good content.&#xA;In original notebook, the Keras library was used. This approach is based on duo Jax + &lt;a href=&#34;https://dm-haiku.readthedocs.io/en/latest/&#34;&gt;Haiku&lt;/a&gt; from DeepMind.&#xA;We will try to distinguish between drug names and Tolkien characters&#xA;using simple LSTM model, surprisingly it isn&amp;rsquo;t as easy as one can think. If you want to challenge yourself, here is popular website with &lt;a href=&#34;https://antidepressantsortolkien.vercel.app/&#34;&gt;great quiz&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
